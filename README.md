# Тестовое задание в Mindbox на вакансию SRE\DevOps-инженер (стажёр).

```yaml
# Здравствуйте, HR и SRE команды Mindbox!
# Меня зовут Дьяченко Юрий, и это тестовое задание на вакансию SRE\DevOps-инженер (стажёр).

apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  labels:
    app: app
spec:
  # Начнём с количества реплик. 
  # Да, нагрузочное тестирование показало, что 4 пода справляются с пиковой нагрузкой, 
  # но 5 мне кажется более правильным решением по следующим причинам:
  #   1. это тестирование, то есть реальная нагрузка может быть и больше, в таком случае лучше иметь запас;
  #   2. так как у нас пять нод, в целях обеспечения отказоустойчивости круто было бы разнести по поду на каждую ноду, 
  #   ведь, если упадёт одна нода при 5 репликах, это не повлияет на общую работоспособность приложения, 
  #   а если у нас пик, и реплик изначально 4, и отказывает узел?
  replicas: 5
  selector:
    matchLabels:
      app: app
  template:
    metadata:
      labels:
        app: app
    spec:
      affinity:
        # Помните про идею разнести по поду на ноду? Вот реализация :)
        # Я думаю всё таки это правило должно быть рекомендательным, нежели обязательным, 
        # потому что не хочется чтобы какие-то поды висели в Pending.
        # Это я, так сказать, балансирую решение, принятое выше, на случай, 
        # если одна или даже две ноды будут недоступны, поды всё равно будут размещены на доступных уздах. 
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - app
                topologyKey: "kubernetes.io/hostname"
      containers:
        - name: app
          image: app:latest
          # Далее про ресурсы.
          # В этом блоке экономим на реквестах, гуляем на лимитах.
          resources:
            # Так как в основном потребление ресурсов ровное и держится на 0.1 CPU и 128M, то такие реквесты и будут.
            # Эта планка будет держаться ночью.
            requests:
              cpu: 100m
              memory: 128Mi
            # Лимиты обеспечат нам свободу по ресурсам в первых запросах и на пиковых нагрузках днём.
            limits:
              cpu: 1
              # Так как наказание за превышение лимита по memory не тротлинг, а OOM - поставим двойной запас.
              memory: 256Mi
          # Пригодится проверка на то, готово ли реально приложение принимать трафик.
          # Зная, что ему для старта требуется 5-10 секунд ставим первую пробу на 10, а остальные с интервалом в 5.
          readinessProbe:
            httpGet:
              path: /healthz
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 5
          # Также не лишним будет обеспечить перезапуск при сбоях.
          livenessProbe:
            httpGet:
              path: /healthz
              port: 80
            initialDelaySeconds: 20
            periodSeconds: 20
          ports:
            - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: app-service
  labels:
    app: app
spec:
  # Используем LoadBalancer, чтобы предоставить доступ к приложению из внешней сети с балансировкой.
  selector:
    app: app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

# Итоги:
#  1. отказоустойчивость обеспечена распределеним реплик на разные ноды (по поду на узел);
#  2. оптимальное потребление ресурсов днём/ночью и первые/последующие запросы реализовано с помощью реквестов и лимитов;
#  3. сбои и преждевременная работа с трафиком минимизированы с помощью проверок.

# Спасибо за прочтение!
```
